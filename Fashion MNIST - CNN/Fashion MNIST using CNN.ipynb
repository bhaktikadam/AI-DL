{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "979226a3",
   "metadata": {},
   "source": [
    "### A Deep Dive into Fashion MNIST with Convolutional Neural Networks (CNN)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7baf0",
   "metadata": {},
   "source": [
    "**Introduction:**<br>\n",
    "In the fast-paced world of technology, machine learning and deep learning have become essential tools for solving complex problems and making sense of vast amounts of data. One fascinating application is image recognition, where Convolutional Neural Networks (CNNs) have proven to be remarkably effective. In this blog post, we'll explore the Fashion MNIST dataset and delve into the creation of a CNN model to classify fashion items with high accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91739191",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example = Fashion Mnist Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117ee432",
   "metadata": {},
   "source": [
    "#### Steps:\n",
    "1. Load Dataset\n",
    "2. EDA\n",
    "3. Data cleaning \n",
    "4. Data Pre processing\n",
    "5. Apply CNN\n",
    "6. Deep Neural Network\n",
    "7. Evaluate the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13dbd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb07bfac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Dataset\n",
    "from tensorflow.keras.datasets import fashion_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b79a7145",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cc4dcc",
   "metadata": {},
   "source": [
    "| Label | Description |\n",
    "|:-----:|-------------|\n",
    "|   0   | T-shirt/top |\n",
    "|   1   | Trouser     |\n",
    "|   2   | Pullover    |\n",
    "|   3   | Dress       |\n",
    "|   4   | Coat        |\n",
    "|   5   | Sandal      |\n",
    "|   6   | Shirt       |\n",
    "|   7   | Sneaker     |\n",
    "|   8   | Bag         |\n",
    "|   9   | Ankle boot  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "211f7b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((60000, 28, 28), (60000,)), ((10000, 28, 28), (10000,)))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(x_train.shape, y_train.shape), (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9480dfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   3,   1,   0,   0,   7,   0,  37,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          1,   2,   0,  27,  84,  11,   0,   0,   0,   0,   0,   0, 119,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          1,   0,   0,  88, 143, 110,   0,   0,   0,   0,  22,  93, 106,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          4,   0,  53, 129, 120, 147, 175, 157, 166, 135, 154, 168, 140,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   2,\n",
       "          0,  11, 137, 130, 128, 160, 176, 159, 167, 178, 149, 151, 144,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   1,   0,   2,   1,   0,   3,   0,\n",
       "          0, 115, 114, 106, 137, 168, 153, 156, 165, 167, 143, 157, 158,\n",
       "         11,   0],\n",
       "       [  0,   0,   0,   0,   1,   0,   0,   0,   0,   0,   3,   0,   0,\n",
       "         89, 139,  90,  94, 153, 149, 131, 151, 169, 172, 143, 159, 169,\n",
       "         48,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   2,   4,   1,   0,   0,   0,  98,\n",
       "        136, 110, 109, 110, 162, 135, 144, 149, 159, 167, 144, 158, 169,\n",
       "        119,   0],\n",
       "       [  0,   0,   2,   2,   1,   2,   0,   0,   0,   0,  26, 108, 117,\n",
       "         99, 111, 117, 136, 156, 134, 154, 154, 156, 160, 141, 147, 156,\n",
       "        178,   0],\n",
       "       [  3,   0,   0,   0,   0,   0,   0,  21,  53,  92, 117, 111, 103,\n",
       "        115, 129, 134, 143, 154, 165, 170, 154, 151, 154, 143, 138, 150,\n",
       "        165,  43],\n",
       "       [  0,   0,  23,  54,  65,  76,  85, 118, 128, 123, 111, 113, 118,\n",
       "        127, 125, 139, 133, 136, 160, 140, 155, 161, 144, 155, 172, 161,\n",
       "        189,  62],\n",
       "       [  0,  68,  94,  90, 111, 114, 111, 114, 115, 127, 135, 136, 143,\n",
       "        126, 127, 151, 154, 143, 148, 125, 162, 162, 144, 138, 153, 162,\n",
       "        196,  58],\n",
       "       [ 70, 169, 129, 104,  98, 100,  94,  97,  98, 102, 108, 106, 119,\n",
       "        120, 129, 149, 156, 167, 190, 190, 196, 198, 198, 187, 197, 189,\n",
       "        184,  36],\n",
       "       [ 16, 126, 171, 188, 188, 184, 171, 153, 135, 120, 126, 127, 146,\n",
       "        185, 195, 209, 208, 255, 209, 177, 245, 252, 251, 251, 247, 220,\n",
       "        206,  49],\n",
       "       [  0,   0,   0,  12,  67, 106, 164, 185, 199, 210, 211, 210, 208,\n",
       "        190, 150,  82,   8,   0,   0,   0, 178, 208, 188, 175, 162, 158,\n",
       "        151,  11],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee91fe97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57606055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19931f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre Processing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6afdbb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/255\n",
    "x_test = x_test/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "00b6cd0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# categorical => One hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72e6dfd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9cde8f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = to_categorical(y_train, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6469c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytest = to_categorical(y_test, num_classes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2da4d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 1.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1928c9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a CNN  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8adf62cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPool2D, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfd10fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam, Adagrad, RMSprop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fada835e",
   "metadata": {},
   "source": [
    "##### CNN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4684cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN layer 1  + CNN Layer 2 => Fatten => Hidden Layer 1 + Hidden Layer 2 => Output Layer\n",
    "# Layer : input data => Convolution with Kernel => Pooling => Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a4faf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4a50531",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a869ab45",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CNN layer 1 \n",
    "\n",
    "model.add(Conv2D(128, (3,3), activation='relu', input_shape = (28,28, 1), padding = 'valid'))\n",
    "model.add(MaxPool2D( pool_size= (2,2)))\n",
    "model.add(Dropout(rate=0.20))\n",
    "\n",
    "# CNN layer 2 \n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size= (2,2)))\n",
    "model.add(Dropout(rate=0.20))\n",
    "\n",
    "# CNN layer 3 \n",
    "model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(MaxPool2D(pool_size= (2,2)))\n",
    "model.add(Dropout(rate=0.20))\n",
    "\n",
    "# Flatten the data\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "# Deep Neural network\n",
    "\n",
    "#hidden layer\n",
    "\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "#hidden layer2\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "# output layer \n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "139af971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 26, 26, 128)       1280      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 13, 13, 128)      0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 13, 13, 128)       0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 11, 11, 64)        73792     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 5, 5, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 5, 5, 64)          0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 3, 3, 32)          18464     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 1, 1, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 1, 1, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 32)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              33792     \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 512)               524800    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 657,258\n",
      "Trainable params: 657,258\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d36cf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "\n",
    "model.compile(optimizer='Adam', loss = tf.keras.losses.categorical_crossentropy, metrics= ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f47df12a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 192s 101ms/step - loss: 0.7056 - accuracy: 0.7307 - val_loss: 0.4760 - val_accuracy: 0.8219\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 192s 102ms/step - loss: 0.5060 - accuracy: 0.8125 - val_loss: 0.4299 - val_accuracy: 0.8410\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 188s 100ms/step - loss: 0.4516 - accuracy: 0.8343 - val_loss: 0.3887 - val_accuracy: 0.8550\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 181s 96ms/step - loss: 0.4217 - accuracy: 0.8452 - val_loss: 0.3866 - val_accuracy: 0.8576\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 175s 94ms/step - loss: 0.4018 - accuracy: 0.8542 - val_loss: 0.3661 - val_accuracy: 0.8638\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 180s 96ms/step - loss: 0.3893 - accuracy: 0.8573 - val_loss: 0.3664 - val_accuracy: 0.8645\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 176s 94ms/step - loss: 0.3847 - accuracy: 0.8590 - val_loss: 0.3563 - val_accuracy: 0.8689\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 180s 96ms/step - loss: 0.3742 - accuracy: 0.8640 - val_loss: 0.3590 - val_accuracy: 0.8697\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 181s 97ms/step - loss: 0.3708 - accuracy: 0.8644 - val_loss: 0.3413 - val_accuracy: 0.8742\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 183s 98ms/step - loss: 0.3652 - accuracy: 0.8665 - val_loss: 0.3334 - val_accuracy: 0.8799\n"
     ]
    }
   ],
   "source": [
    "# train the model \n",
    "\n",
    "result = model.fit(x_train, ytrain, epochs=10, validation_data= (x_test, ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75d3d588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluating model  with test image "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f5aa82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "80b524b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#im1 = Image.open('shirt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9932ee99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#im1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4cc0f54d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre process the test image\n",
    "\n",
    "# 1. convert to grayscale image\n",
    "# 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2859a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\hp\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (4.8.1.78)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\hp\\anaconda3\\envs\\tensorflow_env\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4f7d8def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a0d1e9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1 = cv2.imread('shirt.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb9995b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8770d5d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e7f0e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert color to grayscale\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d2f10b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_g = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9b39912c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize(120 X  135) Image to (28 X 28) pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "15e4fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "im1_new = cv2.resize(im1_g, ([28,28]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "342c362a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 28, 28)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change dimnesion\n",
    "im1_new = np.expand_dims(im1_new, axis = 0)\n",
    "im1_new.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4fcf0a7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#im1_new = np.array(im1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4cc0fde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predict the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d9d13169",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(im1_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0e51ac8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7.0901483e-01, 4.9792214e-33, 2.9098520e-01, 2.0872216e-21,\n",
       "        1.7288019e-12, 0.0000000e+00, 1.6693124e-19, 0.0000000e+00,\n",
       "        3.4353009e-16, 0.0000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cb33b6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "classlabels = {0:'T-shirt/top',1:'Trouser',2:'Pullover',3:'Dress',4:'Coat',5:'Sandal',6:'Shirt',7:'Sneaker',8:'Bag',9:'Ankle boot'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "681de852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "max_index = np.argmax(res) \n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "151ae2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The classlabel for the given image is  T-shirt/top\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,10):\n",
    "    if i == max_index:\n",
    "        print(\"The classlabel for the given image is \", classlabels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a39c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "Conclusion:\n",
    "The MNIST project with deep learning encapsulates the essence of image classification using neural networks. From the initial data collection to the intricate process of model building, training, and testing, each step contributes to the model's ability to recognize and classify handwritten digits. As we navigate the complexities of digit recognition, the MNIST project exemplifies the transformative potential of deep learning in the realm of artificial intelligence. The fusion of data science and neural networks in projects like MNIST paves the way for advancements in image recognition, shaping the future of technology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
