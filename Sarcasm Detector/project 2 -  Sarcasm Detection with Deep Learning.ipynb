{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bb95a79",
   "metadata": {},
   "source": [
    "## Sarcastic Newspaper Headline Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35368dc1",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "**Sarcasm** is a figure of speech or speech comment which is extremely difficult to define. It is often a statement or **comment which means the opposite of what it says.** It may be made with the intent of humour, or it may be made to be hurtful. The basic meaning is to be hostile under the cover of friendliness.\n",
    "\n",
    "Sarcasm detection is a interestring application of natural language processing (NLP) and deep learning. Sarcasm is like a hidden treasure in the vast world of language. It adds a whole new level of complexity that can really test traditional language processing models. To truly understand sarcasm, you need to not only understand the literal meaning of words, but also appreciate the subtle nuances that can turn a simple statement into a sarcastic remark. As we venture into the realm of natural language processing, we dive into the exciting world of detecting sarcasm using the incredible power of deep learning. In this project, Aim is to build a robust sarcasm detection model using deep learning techniques.<br> \n",
    "The project involves various steps, including data analysis, data cleaning, model building, testing, and predicting user's inputs. From the very beginning, where we analyze data, to the final destination of creating a user-friendly model, we navigate through the ups and downs of integrating deep learning into the fascinating domain of linguistic wit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "eb997774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the basic / required libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Remove Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "de53e990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow for model buliding\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "b8051433",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "25cb9d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dsataset\n",
    "df = pd.read_json(\"sarcasm.json\", lines= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "949f1aec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28619, 3)"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef8e0a",
   "metadata": {},
   "source": [
    "####  Data Cleaning\n",
    "Cleaning the data is crucial to ensure the effectiveness of the model. This step involves handling missing values, removing irrelevant information, and addressing any noise in the dataset. Additionally, preprocessing steps like tokenization, removing stop words, and stemming/lemmatization may be applied to convert the raw text into a format suitable for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "46f5c51d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'inclement weather prevents liar from getting to work'"
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['headline'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "bc2b7887",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28614</th>\n",
       "      <td>1</td>\n",
       "      <td>jews to celebrate rosh hashasha or something</td>\n",
       "      <td>https://www.theonion.com/jews-to-celebrate-ros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28615</th>\n",
       "      <td>1</td>\n",
       "      <td>internal affairs investigator disappointed con...</td>\n",
       "      <td>https://local.theonion.com/internal-affairs-in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28616</th>\n",
       "      <td>0</td>\n",
       "      <td>the most beautiful acceptance speech this week...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/andrew-ah...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28617</th>\n",
       "      <td>1</td>\n",
       "      <td>mars probe destroyed by orbiting spielberg-gat...</td>\n",
       "      <td>https://www.theonion.com/mars-probe-destroyed-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28618</th>\n",
       "      <td>1</td>\n",
       "      <td>dad clarifies this not a food stop</td>\n",
       "      <td>https://www.theonion.com/dad-clarifies-this-no...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28619 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       is_sarcastic                                           headline  \\\n",
       "0                 1  thirtysomething scientists unveil doomsday clo...   \n",
       "1                 0  dem rep. totally nails why congress is falling...   \n",
       "2                 0  eat your veggies: 9 deliciously different recipes   \n",
       "3                 1  inclement weather prevents liar from getting t...   \n",
       "4                 1  mother comes pretty close to using word 'strea...   \n",
       "...             ...                                                ...   \n",
       "28614             1       jews to celebrate rosh hashasha or something   \n",
       "28615             1  internal affairs investigator disappointed con...   \n",
       "28616             0  the most beautiful acceptance speech this week...   \n",
       "28617             1  mars probe destroyed by orbiting spielberg-gat...   \n",
       "28618             1                 dad clarifies this not a food stop   \n",
       "\n",
       "                                            article_link  \n",
       "0      https://www.theonion.com/thirtysomething-scien...  \n",
       "1      https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2      https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3      https://local.theonion.com/inclement-weather-p...  \n",
       "4      https://www.theonion.com/mother-comes-pretty-c...  \n",
       "...                                                  ...  \n",
       "28614  https://www.theonion.com/jews-to-celebrate-ros...  \n",
       "28615  https://local.theonion.com/internal-affairs-in...  \n",
       "28616  https://www.huffingtonpost.com/entry/andrew-ah...  \n",
       "28617  https://www.theonion.com/mars-probe-destroyed-...  \n",
       "28618  https://www.theonion.com/dad-clarifies-this-no...  \n",
       "\n",
       "[28619 rows x 3 columns]"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a515f09",
   "metadata": {},
   "source": [
    "Since the **df['article_link']** column is dispensable , will drop it from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "e11511b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels='article_link', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "330b3470",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1\n",
       "1        0\n",
       "2        0\n",
       "3        1\n",
       "4        1\n",
       "        ..\n",
       "28614    1\n",
       "28615    1\n",
       "28616    0\n",
       "28617    1\n",
       "28618    1\n",
       "Name: is_sarcastic, Length: 28619, dtype: int64"
      ]
     },
     "execution_count": 284,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_sarcastic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "58f2c817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sarcastic\n",
       "0    14985\n",
       "1    13634\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_sarcastic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "cbf101a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_sarcastic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a9772fd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        thirtysomething scientists unveil doomsday clo...\n",
       "1        dem rep. totally nails why congress is falling...\n",
       "2        eat your veggies: 9 deliciously different recipes\n",
       "3        inclement weather prevents liar from getting t...\n",
       "4        mother comes pretty close to using word 'strea...\n",
       "                               ...                        \n",
       "28614         jews to celebrate rosh hashasha or something\n",
       "28615    internal affairs investigator disappointed con...\n",
       "28616    the most beautiful acceptance speech this week...\n",
       "28617    mars probe destroyed by orbiting spielberg-gat...\n",
       "28618                   dad clarifies this not a food stop\n",
       "Name: headline, Length: 28619, dtype: object"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0ede3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6969ebf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "22e015a8",
   "metadata": {},
   "source": [
    "**'is_sarcastic'** coloumn of dataframe consist of labels/ output indicating whether respective headlines are sarcastic or not. Here df['is_sarcastic'] column have two viz. 1 and 0 where, **1** indicates that the given headline **is sarcastic** and **0** indicates headline is **not sarcastic.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f423e74",
   "metadata": {},
   "source": [
    "Before performing tokenization on original dataset i tried same process on the smaller dataset to understand it more throughly  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7e3442",
   "metadata": {},
   "source": [
    "I have created one sample list with four semtences in it.<br>\n",
    "I have followed following steps to tikenize the data\n",
    "- Firstly we will try to tokenize the whole list using tensorflow's Tokenizer method. It will assign the index to each unique word in the list. \n",
    "- In next step we will create index sequence list for the each sentance in the defined sample list. \n",
    "- Next step to perform the paading on the obatined sequnces to avoid discrepancy in the results.    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "b7818065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets use natural language processing for string preprocessing\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "0a680c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# removing stopwords\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "9ccf96bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_stopwords = set(stopwords.words())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f037a559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing stopwords\n",
    "#x_data = x.apply(lambda review : [ i       for i in review.split()       if i not in eng_stopwords ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "77948f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = [\"I am learning pythyon\", 'I am learning deep learning', 'I love dogs', 'I love cats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5acdc8f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6365ef6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am learning pythyon',\n",
       " 'I am learning deep learning',\n",
       " 'I love dogs',\n",
       " 'I love cats']"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "1253b861",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(10, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "5de13b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9f110732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 1,\n",
       " 'learning': 2,\n",
       " 'am': 3,\n",
       " 'love': 4,\n",
       " 'pythyon': 5,\n",
       " 'deep': 6,\n",
       " 'dogs': 7,\n",
       " 'cats': 8}"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "8045383a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_sq = token.texts_to_sequences(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "c0049dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 3, 2, 5], [1, 3, 2, 6, 2], [1, 4, 7], [1, 4, 8]]\n"
     ]
    }
   ],
   "source": [
    "print(sent_sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe50523f",
   "metadata": {},
   "source": [
    "#### Model Building\n",
    "The heart of the project lies in building a robust deep learning model for sarcasm detection. Common architectures for natural language processing tasks include recurrent neural networks (RNNs)and long short-term memory networks (LSTMs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "6f10db8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "accc720e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_final = pad_sequences(sent_sq, maxlen=10, truncating= 'post', padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "285429c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3, 2, 5, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 3, 2, 6, 2, 0, 0, 0, 0, 0],\n",
       "       [1, 4, 7, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 4, 8, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3ec87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6014eb4",
   "metadata": {},
   "source": [
    "Now, time to tokenise the origanl dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "979763f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the ease , will convert the dataset into the list \n",
    "hl = df['headline'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "9dee89f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(hl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "5d49da73",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#hl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "d93904af",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df['is_sarcastic'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "5270b013",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " ...]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef4de3",
   "metadata": {},
   "source": [
    "#### Model Training\n",
    "Train the model using the preprocessed dataset. Split the data into training and validation sets to evaluate the model's performance during training. Utilize appropriate loss functions and optimization algorithms. Monitor key metrics such as accuracy,loss to assess the model's performance.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "6dc1cb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing dataset =>> 9:1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b3645b32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25757"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ind = df.shape[0]*90//100 \n",
    "train_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "38d130cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#training dataset\n",
    "headlines_train = hl[ : train_ind]\n",
    "labels_train = labels[ : train_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "6cb69052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing dataset\n",
    "headlines_test = hl[train_ind : ]\n",
    "labels_test = labels[train_ind : ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "0ddb6b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create word oindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e5d0ef02",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = Tokenizer(num_words=1000, oov_token= 'UNK' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "af95ef98",
   "metadata": {},
   "outputs": [],
   "source": [
    "token.fit_on_texts(headlines_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "d9c451d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'UNK': 1,\n",
       " 'to': 2,\n",
       " 'of': 3,\n",
       " 'the': 4,\n",
       " 'in': 5,\n",
       " 'for': 6,\n",
       " 'a': 7,\n",
       " 'on': 8,\n",
       " 'and': 9,\n",
       " 'with': 10,\n",
       " 'is': 11,\n",
       " 'new': 12,\n",
       " 'man': 13,\n",
       " 'trump': 14,\n",
       " 'at': 15,\n",
       " 'from': 16,\n",
       " 'about': 17,\n",
       " 'by': 18,\n",
       " 'you': 19,\n",
       " 'after': 20,\n",
       " 'this': 21,\n",
       " 'be': 22,\n",
       " 'out': 23,\n",
       " 'up': 24,\n",
       " 'as': 25,\n",
       " 'that': 26,\n",
       " 'it': 27,\n",
       " 'how': 28,\n",
       " 'not': 29,\n",
       " 'he': 30,\n",
       " 'his': 31,\n",
       " 'what': 32,\n",
       " 'your': 33,\n",
       " 'are': 34,\n",
       " 'just': 35,\n",
       " 'who': 36,\n",
       " 'has': 37,\n",
       " 'all': 38,\n",
       " 'will': 39,\n",
       " 'report': 40,\n",
       " 'into': 41,\n",
       " 'more': 42,\n",
       " 'have': 43,\n",
       " 'one': 44,\n",
       " 'year': 45,\n",
       " 'over': 46,\n",
       " 'u': 47,\n",
       " 'why': 48,\n",
       " 'area': 49,\n",
       " 'woman': 50,\n",
       " 'can': 51,\n",
       " 'day': 52,\n",
       " 's': 53,\n",
       " 'says': 54,\n",
       " 'first': 55,\n",
       " 'time': 56,\n",
       " 'donald': 57,\n",
       " 'like': 58,\n",
       " 'no': 59,\n",
       " 'get': 60,\n",
       " 'her': 61,\n",
       " 'old': 62,\n",
       " 'off': 63,\n",
       " 'people': 64,\n",
       " 'life': 65,\n",
       " \"trump's\": 66,\n",
       " \"'\": 67,\n",
       " 'now': 68,\n",
       " 'house': 69,\n",
       " 'an': 70,\n",
       " 'obama': 71,\n",
       " 'white': 72,\n",
       " 'still': 73,\n",
       " 'back': 74,\n",
       " 'make': 75,\n",
       " 'was': 76,\n",
       " 'down': 77,\n",
       " 'than': 78,\n",
       " 'women': 79,\n",
       " 'if': 80,\n",
       " 'my': 81,\n",
       " 'could': 82,\n",
       " 'when': 83,\n",
       " 'clinton': 84,\n",
       " 'i': 85,\n",
       " '5': 86,\n",
       " 'they': 87,\n",
       " 'way': 88,\n",
       " 'before': 89,\n",
       " 'world': 90,\n",
       " 'their': 91,\n",
       " 'americans': 92,\n",
       " 'him': 93,\n",
       " 'family': 94,\n",
       " 'we': 95,\n",
       " 'do': 96,\n",
       " 'only': 97,\n",
       " 'study': 98,\n",
       " 'would': 99,\n",
       " 'most': 100,\n",
       " 'black': 101,\n",
       " 'being': 102,\n",
       " 'gop': 103,\n",
       " 'so': 104,\n",
       " 'school': 105,\n",
       " 'years': 106,\n",
       " 'bill': 107,\n",
       " 'best': 108,\n",
       " 'know': 109,\n",
       " 'finds': 110,\n",
       " 'really': 111,\n",
       " \"it's\": 112,\n",
       " 'should': 113,\n",
       " 'last': 114,\n",
       " \"can't\": 115,\n",
       " '3': 116,\n",
       " 'american': 117,\n",
       " 'nation': 118,\n",
       " 'watch': 119,\n",
       " 'she': 120,\n",
       " '10': 121,\n",
       " 'but': 122,\n",
       " 'police': 123,\n",
       " 'going': 124,\n",
       " 'home': 125,\n",
       " 'video': 126,\n",
       " 'president': 127,\n",
       " 'death': 128,\n",
       " 'during': 129,\n",
       " 'good': 130,\n",
       " 'say': 131,\n",
       " 'state': 132,\n",
       " 'or': 133,\n",
       " 'show': 134,\n",
       " 'health': 135,\n",
       " 'against': 136,\n",
       " 'mom': 137,\n",
       " 'getting': 138,\n",
       " 'right': 139,\n",
       " 'campaign': 140,\n",
       " 'every': 141,\n",
       " \"'the\": 142,\n",
       " 'too': 143,\n",
       " 'things': 144,\n",
       " 'big': 145,\n",
       " 'some': 146,\n",
       " '000': 147,\n",
       " 'gets': 148,\n",
       " '2': 149,\n",
       " 'party': 150,\n",
       " 'self': 151,\n",
       " 'work': 152,\n",
       " 'hillary': 153,\n",
       " 'while': 154,\n",
       " 'need': 155,\n",
       " 'parents': 156,\n",
       " 'may': 157,\n",
       " 'love': 158,\n",
       " 'never': 159,\n",
       " 'where': 160,\n",
       " 'little': 161,\n",
       " 'own': 162,\n",
       " 'through': 163,\n",
       " 'john': 164,\n",
       " 'child': 165,\n",
       " 'other': 166,\n",
       " 'take': 167,\n",
       " 'court': 168,\n",
       " 'kids': 169,\n",
       " \"doesn't\": 170,\n",
       " 'calls': 171,\n",
       " 'these': 172,\n",
       " 'news': 173,\n",
       " 'makes': 174,\n",
       " 'next': 175,\n",
       " 'high': 176,\n",
       " 'change': 177,\n",
       " 'dead': 178,\n",
       " 'election': 179,\n",
       " 'local': 180,\n",
       " 'stop': 181,\n",
       " '4': 182,\n",
       " '7': 183,\n",
       " \"he's\": 184,\n",
       " 'gay': 185,\n",
       " \"don't\": 186,\n",
       " 'want': 187,\n",
       " 'see': 188,\n",
       " 'takes': 189,\n",
       " 'war': 190,\n",
       " 'our': 191,\n",
       " 'even': 192,\n",
       " 'go': 193,\n",
       " 'real': 194,\n",
       " \"here's\": 195,\n",
       " 'look': 196,\n",
       " 'around': 197,\n",
       " 'its': 198,\n",
       " 'baby': 199,\n",
       " 'america': 200,\n",
       " 'them': 201,\n",
       " 'bush': 202,\n",
       " \"nation's\": 203,\n",
       " 'guy': 204,\n",
       " 'made': 205,\n",
       " 'two': 206,\n",
       " 'office': 207,\n",
       " 'again': 208,\n",
       " 'got': 209,\n",
       " 'dog': 210,\n",
       " 'sex': 211,\n",
       " '6': 212,\n",
       " 'million': 213,\n",
       " 'plan': 214,\n",
       " 'dad': 215,\n",
       " 'ever': 216,\n",
       " 'college': 217,\n",
       " 'much': 218,\n",
       " 'been': 219,\n",
       " 'help': 220,\n",
       " 'another': 221,\n",
       " 'debate': 222,\n",
       " 'finally': 223,\n",
       " 'week': 224,\n",
       " 'wants': 225,\n",
       " 'announces': 226,\n",
       " 'long': 227,\n",
       " 'job': 228,\n",
       " 'gun': 229,\n",
       " 'thing': 230,\n",
       " 'night': 231,\n",
       " 'reveals': 232,\n",
       " 'care': 233,\n",
       " 'there': 234,\n",
       " '1': 235,\n",
       " 'live': 236,\n",
       " 'actually': 237,\n",
       " 'money': 238,\n",
       " 'under': 239,\n",
       " 'couple': 240,\n",
       " '9': 241,\n",
       " 'us': 242,\n",
       " 'congress': 243,\n",
       " 'senate': 244,\n",
       " 'better': 245,\n",
       " 'shows': 246,\n",
       " 'sexual': 247,\n",
       " \"man's\": 248,\n",
       " 'national': 249,\n",
       " 'god': 250,\n",
       " 'without': 251,\n",
       " 'everyone': 252,\n",
       " 'north': 253,\n",
       " 'trying': 254,\n",
       " 'any': 255,\n",
       " 'had': 256,\n",
       " 'star': 257,\n",
       " 'me': 258,\n",
       " 'facebook': 259,\n",
       " '8': 260,\n",
       " 'paul': 261,\n",
       " 'bad': 262,\n",
       " 'top': 263,\n",
       " 'enough': 264,\n",
       " 'face': 265,\n",
       " 'anti': 266,\n",
       " 'shooting': 267,\n",
       " 'food': 268,\n",
       " 'ways': 269,\n",
       " 'season': 270,\n",
       " 'give': 271,\n",
       " \"won't\": 272,\n",
       " 'climate': 273,\n",
       " 'teen': 274,\n",
       " 'making': 275,\n",
       " 'men': 276,\n",
       " '20': 277,\n",
       " 'game': 278,\n",
       " 'media': 279,\n",
       " 'part': 280,\n",
       " 'history': 281,\n",
       " 'business': 282,\n",
       " 'movie': 283,\n",
       " 'free': 284,\n",
       " 'body': 285,\n",
       " 'supreme': 286,\n",
       " 'end': 287,\n",
       " 'away': 288,\n",
       " 'single': 289,\n",
       " 'york': 290,\n",
       " 'introduces': 291,\n",
       " 'city': 292,\n",
       " 'law': 293,\n",
       " 'story': 294,\n",
       " 'think': 295,\n",
       " 'fight': 296,\n",
       " 'son': 297,\n",
       " 'deal': 298,\n",
       " 'students': 299,\n",
       " 'pope': 300,\n",
       " 'second': 301,\n",
       " '11': 302,\n",
       " 'friend': 303,\n",
       " 'already': 304,\n",
       " 'tell': 305,\n",
       " 'children': 306,\n",
       " 'friends': 307,\n",
       " 'attack': 308,\n",
       " 'releases': 309,\n",
       " 'fire': 310,\n",
       " 'tv': 311,\n",
       " 'car': 312,\n",
       " 'found': 313,\n",
       " 'same': 314,\n",
       " 'entire': 315,\n",
       " 'presidential': 316,\n",
       " 'girl': 317,\n",
       " 'must': 318,\n",
       " 'wedding': 319,\n",
       " 'power': 320,\n",
       " 'line': 321,\n",
       " 'company': 322,\n",
       " 'public': 323,\n",
       " 'book': 324,\n",
       " 'pretty': 325,\n",
       " 'former': 326,\n",
       " 'film': 327,\n",
       " 'run': 328,\n",
       " 'support': 329,\n",
       " 'great': 330,\n",
       " 'government': 331,\n",
       " 'having': 332,\n",
       " 'come': 333,\n",
       " 'sanders': 334,\n",
       " 'find': 335,\n",
       " 'talk': 336,\n",
       " \"didn't\": 337,\n",
       " 'coming': 338,\n",
       " 'morning': 339,\n",
       " 'scientists': 340,\n",
       " 'unveils': 341,\n",
       " 'call': 342,\n",
       " 'room': 343,\n",
       " 'does': 344,\n",
       " 'behind': 345,\n",
       " 'social': 346,\n",
       " 'use': 347,\n",
       " 'name': 348,\n",
       " 'security': 349,\n",
       " 'doing': 350,\n",
       " 'between': 351,\n",
       " 'republican': 352,\n",
       " 'open': 353,\n",
       " 'photos': 354,\n",
       " 'middle': 355,\n",
       " 'keep': 356,\n",
       " 'student': 357,\n",
       " 'james': 358,\n",
       " 'looking': 359,\n",
       " 'might': 360,\n",
       " 'asks': 361,\n",
       " 'case': 362,\n",
       " 'because': 363,\n",
       " 'future': 364,\n",
       " 'fans': 365,\n",
       " 'email': 366,\n",
       " 'ceo': 367,\n",
       " 'republicans': 368,\n",
       " 'speech': 369,\n",
       " 'full': 370,\n",
       " 'fucking': 371,\n",
       " 'admits': 372,\n",
       " 'christmas': 373,\n",
       " 'group': 374,\n",
       " 'win': 375,\n",
       " 'thinks': 376,\n",
       " 'human': 377,\n",
       " 'poll': 378,\n",
       " \"world's\": 379,\n",
       " 'claims': 380,\n",
       " '12': 381,\n",
       " 'michael': 382,\n",
       " 'rights': 383,\n",
       " 'tax': 384,\n",
       " 'person': 385,\n",
       " 'put': 386,\n",
       " 'marriage': 387,\n",
       " 'control': 388,\n",
       " 'once': 389,\n",
       " 'voters': 390,\n",
       " 'team': 391,\n",
       " 'vote': 392,\n",
       " 'department': 393,\n",
       " 'violence': 394,\n",
       " '2016': 395,\n",
       " 'something': 396,\n",
       " 'eating': 397,\n",
       " 'ryan': 398,\n",
       " 'country': 399,\n",
       " 'female': 400,\n",
       " 'killed': 401,\n",
       " 'ad': 402,\n",
       " 'forced': 403,\n",
       " 'always': 404,\n",
       " 'dies': 405,\n",
       " 'sure': 406,\n",
       " 'until': 407,\n",
       " 'used': 408,\n",
       " 'goes': 409,\n",
       " 'head': 410,\n",
       " 'ban': 411,\n",
       " 'inside': 412,\n",
       " 'hot': 413,\n",
       " 'father': 414,\n",
       " 'secret': 415,\n",
       " 'super': 416,\n",
       " 'plans': 417,\n",
       " 'judge': 418,\n",
       " 'bernie': 419,\n",
       " 'photo': 420,\n",
       " 'thousands': 421,\n",
       " 'political': 422,\n",
       " 'post': 423,\n",
       " 'democrats': 424,\n",
       " 'minutes': 425,\n",
       " 'water': 426,\n",
       " 'past': 427,\n",
       " 'save': 428,\n",
       " 'meet': 429,\n",
       " 'let': 430,\n",
       " 'each': 431,\n",
       " 'taking': 432,\n",
       " 'candidate': 433,\n",
       " 'living': 434,\n",
       " 'very': 435,\n",
       " 'many': 436,\n",
       " 'running': 437,\n",
       " 'left': 438,\n",
       " 'music': 439,\n",
       " 'here': 440,\n",
       " 'mother': 441,\n",
       " 'boy': 442,\n",
       " 'warns': 443,\n",
       " 'perfect': 444,\n",
       " 'race': 445,\n",
       " 'teacher': 446,\n",
       " 'list': 447,\n",
       " '15': 448,\n",
       " 'pay': 449,\n",
       " 'george': 450,\n",
       " 'service': 451,\n",
       " 'missing': 452,\n",
       " 'were': 453,\n",
       " 'red': 454,\n",
       " 'days': 455,\n",
       " 'art': 456,\n",
       " 'summer': 457,\n",
       " \"'i\": 458,\n",
       " 'wall': 459,\n",
       " 'times': 460,\n",
       " 'idea': 461,\n",
       " 'working': 462,\n",
       " 'california': 463,\n",
       " \"you're\": 464,\n",
       " 'month': 465,\n",
       " 'heart': 466,\n",
       " 'reports': 467,\n",
       " 'states': 468,\n",
       " 'record': 469,\n",
       " 'tells': 470,\n",
       " 'secretary': 471,\n",
       " 'looks': 472,\n",
       " 'wife': 473,\n",
       " 'age': 474,\n",
       " 'wrong': 475,\n",
       " 'twitter': 476,\n",
       " 'start': 477,\n",
       " 'hours': 478,\n",
       " 'mike': 479,\n",
       " 'place': 480,\n",
       " 'everything': 481,\n",
       " 'class': 482,\n",
       " 'comes': 483,\n",
       " 'wearing': 484,\n",
       " 'lost': 485,\n",
       " 'lives': 486,\n",
       " 'set': 487,\n",
       " 'russia': 488,\n",
       " 'shot': 489,\n",
       " 'employee': 490,\n",
       " 'phone': 491,\n",
       " 'three': 492,\n",
       " 'texas': 493,\n",
       " 'needs': 494,\n",
       " 'yet': 495,\n",
       " 'town': 496,\n",
       " 'meeting': 497,\n",
       " 'ready': 498,\n",
       " 'someone': 499,\n",
       " '50': 500,\n",
       " 'thought': 501,\n",
       " 'gives': 502,\n",
       " 'kill': 503,\n",
       " 'daughter': 504,\n",
       " 'obamacare': 505,\n",
       " 'young': 506,\n",
       " 'talks': 507,\n",
       " '30': 508,\n",
       " 'iran': 509,\n",
       " 'believe': 510,\n",
       " 'did': 511,\n",
       " 'cat': 512,\n",
       " 'isis': 513,\n",
       " 'probably': 514,\n",
       " 'small': 515,\n",
       " 'shit': 516,\n",
       " 'drug': 517,\n",
       " 'prison': 518,\n",
       " 'chief': 519,\n",
       " 'cancer': 520,\n",
       " 'street': 521,\n",
       " \"i'm\": 522,\n",
       " 'together': 523,\n",
       " 'cruz': 524,\n",
       " 'king': 525,\n",
       " 'ice': 526,\n",
       " 'giving': 527,\n",
       " 'kim': 528,\n",
       " 'internet': 529,\n",
       " 'dream': 530,\n",
       " 'letter': 531,\n",
       " 'fan': 532,\n",
       " 'democratic': 533,\n",
       " 'south': 534,\n",
       " 'justice': 535,\n",
       " 'ex': 536,\n",
       " 'earth': 537,\n",
       " 'crisis': 538,\n",
       " 'questions': 539,\n",
       " 'third': 540,\n",
       " 'talking': 541,\n",
       " 'korea': 542,\n",
       " 'few': 543,\n",
       " 'half': 544,\n",
       " 'officials': 545,\n",
       " 'breaking': 546,\n",
       " 'today': 547,\n",
       " 'nothing': 548,\n",
       " 'attacks': 549,\n",
       " 'girlfriend': 550,\n",
       " 'mark': 551,\n",
       " 'using': 552,\n",
       " 'less': 553,\n",
       " 'romney': 554,\n",
       " 'those': 555,\n",
       " \"she's\": 556,\n",
       " 'personal': 557,\n",
       " 't': 558,\n",
       " 'hard': 559,\n",
       " 'months': 560,\n",
       " 'percent': 561,\n",
       " 'word': 562,\n",
       " 'chris': 563,\n",
       " 'wins': 564,\n",
       " 'watching': 565,\n",
       " 'administration': 566,\n",
       " 'feel': 567,\n",
       " 'director': 568,\n",
       " 'restaurant': 569,\n",
       " 'biden': 570,\n",
       " 'community': 571,\n",
       " 'air': 572,\n",
       " 'owner': 573,\n",
       " 'nuclear': 574,\n",
       " 'hour': 575,\n",
       " 'latest': 576,\n",
       " 'move': 577,\n",
       " 'system': 578,\n",
       " 'rock': 579,\n",
       " \"what's\": 580,\n",
       " \"women's\": 581,\n",
       " 'tips': 582,\n",
       " 'sleep': 583,\n",
       " 'gift': 584,\n",
       " 'abortion': 585,\n",
       " 'birthday': 586,\n",
       " 'leaves': 587,\n",
       " 'military': 588,\n",
       " 'outside': 589,\n",
       " 'whole': 590,\n",
       " 'order': 591,\n",
       " 'fbi': 592,\n",
       " 'buy': 593,\n",
       " 'education': 594,\n",
       " 'since': 595,\n",
       " 'kind': 596,\n",
       " 'kid': 597,\n",
       " 'waiting': 598,\n",
       " 'knows': 599,\n",
       " 'told': 600,\n",
       " 'special': 601,\n",
       " 'leave': 602,\n",
       " 'bar': 603,\n",
       " 'florida': 604,\n",
       " 'tweets': 605,\n",
       " 'well': 606,\n",
       " 'offers': 607,\n",
       " 'minute': 608,\n",
       " 'excited': 609,\n",
       " 'guide': 610,\n",
       " 'washington': 611,\n",
       " 'happy': 612,\n",
       " '100': 613,\n",
       " 'different': 614,\n",
       " 'non': 615,\n",
       " 'called': 616,\n",
       " 'thinking': 617,\n",
       " 'ted': 618,\n",
       " 'lot': 619,\n",
       " 'box': 620,\n",
       " 'mueller': 621,\n",
       " 'march': 622,\n",
       " 'francis': 623,\n",
       " 'response': 624,\n",
       " 'following': 625,\n",
       " 'straight': 626,\n",
       " 'federal': 627,\n",
       " 'worried': 628,\n",
       " 'immigration': 629,\n",
       " 'stephen': 630,\n",
       " 'majority': 631,\n",
       " 'millions': 632,\n",
       " 'fox': 633,\n",
       " 'career': 634,\n",
       " 'muslim': 635,\n",
       " 'issues': 636,\n",
       " 'celebrates': 637,\n",
       " 'front': 638,\n",
       " 'store': 639,\n",
       " 'beautiful': 640,\n",
       " 'assault': 641,\n",
       " 'hit': 642,\n",
       " 'read': 643,\n",
       " 'online': 644,\n",
       " 'david': 645,\n",
       " 'cover': 646,\n",
       " 'reason': 647,\n",
       " 'late': 648,\n",
       " 'trailer': 649,\n",
       " '2015': 650,\n",
       " 'russian': 651,\n",
       " 'ask': 652,\n",
       " 'problem': 653,\n",
       " 'himself': 654,\n",
       " 'anything': 655,\n",
       " 'rise': 656,\n",
       " 'rules': 657,\n",
       " 'billion': 658,\n",
       " 'leaders': 659,\n",
       " 'taylor': 660,\n",
       " 'fun': 661,\n",
       " 'hate': 662,\n",
       " 'congressman': 663,\n",
       " 'china': 664,\n",
       " 'drunk': 665,\n",
       " 'visit': 666,\n",
       " 'chinese': 667,\n",
       " 'feels': 668,\n",
       " 'birth': 669,\n",
       " 'huge': 670,\n",
       " 'series': 671,\n",
       " 'scott': 672,\n",
       " 'opens': 673,\n",
       " 'moment': 674,\n",
       " 'girls': 675,\n",
       " 'holiday': 676,\n",
       " 'investigation': 677,\n",
       " 'relationship': 678,\n",
       " 'spends': 679,\n",
       " 'senator': 680,\n",
       " 'hair': 681,\n",
       " \"isn't\": 682,\n",
       " 'protest': 683,\n",
       " \"america's\": 684,\n",
       " '40': 685,\n",
       " 'travel': 686,\n",
       " 'break': 687,\n",
       " 'experts': 688,\n",
       " 'hollywood': 689,\n",
       " 'union': 690,\n",
       " 'al': 691,\n",
       " 'cop': 692,\n",
       " 'pence': 693,\n",
       " 'candidates': 694,\n",
       " 'victims': 695,\n",
       " 'accused': 696,\n",
       " 'die': 697,\n",
       " 'host': 698,\n",
       " 'whether': 699,\n",
       " \"obama's\": 700,\n",
       " 'across': 701,\n",
       " 'huffpost': 702,\n",
       " 'play': 703,\n",
       " 'policy': 704,\n",
       " 'tom': 705,\n",
       " 'bring': 706,\n",
       " 'experience': 707,\n",
       " 'massive': 708,\n",
       " 'hands': 709,\n",
       " 'apple': 710,\n",
       " 'politics': 711,\n",
       " 'early': 712,\n",
       " 'starting': 713,\n",
       " 'light': 714,\n",
       " 'discover': 715,\n",
       " 'favorite': 716,\n",
       " 'center': 717,\n",
       " 'dating': 718,\n",
       " 'killing': 719,\n",
       " 'date': 720,\n",
       " 'totally': 721,\n",
       " 'stars': 722,\n",
       " 'leader': 723,\n",
       " 'weekend': 724,\n",
       " 'mass': 725,\n",
       " 'c': 726,\n",
       " 'hurricane': 727,\n",
       " 'trip': 728,\n",
       " 'k': 729,\n",
       " 'final': 730,\n",
       " 'united': 731,\n",
       " 'learned': 732,\n",
       " 'message': 733,\n",
       " 'fall': 734,\n",
       " 'iraq': 735,\n",
       " 'joe': 736,\n",
       " 'point': 737,\n",
       " 'struggling': 738,\n",
       " 'reasons': 739,\n",
       " 'j': 740,\n",
       " 'oil': 741,\n",
       " 'powerful': 742,\n",
       " 'clearly': 743,\n",
       " 'become': 744,\n",
       " 'least': 745,\n",
       " 'turns': 746,\n",
       " \"they're\": 747,\n",
       " 'dance': 748,\n",
       " 'crash': 749,\n",
       " 'check': 750,\n",
       " 'adds': 751,\n",
       " 'key': 752,\n",
       " 'stand': 753,\n",
       " 'reality': 754,\n",
       " 'lessons': 755,\n",
       " 'sick': 756,\n",
       " 'signs': 757,\n",
       " 'worst': 758,\n",
       " 'true': 759,\n",
       " 'fuck': 760,\n",
       " 'role': 761,\n",
       " 'begins': 762,\n",
       " 'anniversary': 763,\n",
       " 'puts': 764,\n",
       " 'feeling': 765,\n",
       " 'lose': 766,\n",
       " 'abuse': 767,\n",
       " 'song': 768,\n",
       " 'completely': 769,\n",
       " 'fashion': 770,\n",
       " 'turn': 771,\n",
       " 'employees': 772,\n",
       " 'sports': 773,\n",
       " 'apartment': 774,\n",
       " 'moving': 775,\n",
       " 'adorable': 776,\n",
       " 'global': 777,\n",
       " 'keeps': 778,\n",
       " 'hoping': 779,\n",
       " 'almost': 780,\n",
       " 'returns': 781,\n",
       " 'wishes': 782,\n",
       " 'prince': 783,\n",
       " 'words': 784,\n",
       " 'vows': 785,\n",
       " 'hope': 786,\n",
       " 'jimmy': 787,\n",
       " 'sign': 788,\n",
       " 'un': 789,\n",
       " 'far': 790,\n",
       " 'driving': 791,\n",
       " 'dinner': 792,\n",
       " 'number': 793,\n",
       " 'hall': 794,\n",
       " 'announce': 795,\n",
       " 'space': 796,\n",
       " 'risk': 797,\n",
       " 'side': 798,\n",
       " 'longer': 799,\n",
       " 'return': 800,\n",
       " 'hand': 801,\n",
       " 'breaks': 802,\n",
       " 'amazon': 803,\n",
       " 'murder': 804,\n",
       " 'nfl': 805,\n",
       " 'robert': 806,\n",
       " 'lgbt': 807,\n",
       " 'mind': 808,\n",
       " 'seen': 809,\n",
       " 'syrian': 810,\n",
       " 'west': 811,\n",
       " 'd': 812,\n",
       " 'bus': 813,\n",
       " 'interview': 814,\n",
       " 'apologizes': 815,\n",
       " 'worth': 816,\n",
       " 'official': 817,\n",
       " 'syria': 818,\n",
       " 'kills': 819,\n",
       " 'oscar': 820,\n",
       " 'cops': 821,\n",
       " 'mental': 822,\n",
       " 'audience': 823,\n",
       " 'evidence': 824,\n",
       " 'low': 825,\n",
       " 'halloween': 826,\n",
       " 'press': 827,\n",
       " 'demands': 828,\n",
       " 'cut': 829,\n",
       " 'biggest': 830,\n",
       " 'five': 831,\n",
       " 'playing': 832,\n",
       " 'lead': 833,\n",
       " 'data': 834,\n",
       " 'coffee': 835,\n",
       " 'weird': 836,\n",
       " 'governor': 837,\n",
       " 'stage': 838,\n",
       " 'important': 839,\n",
       " 'accidentally': 840,\n",
       " 'conversation': 841,\n",
       " 'jr': 842,\n",
       " 'chance': 843,\n",
       " 'cool': 844,\n",
       " 'schools': 845,\n",
       " 'which': 846,\n",
       " \"there's\": 847,\n",
       " 'reportedly': 848,\n",
       " 'suspect': 849,\n",
       " 'near': 850,\n",
       " 'users': 851,\n",
       " 'success': 852,\n",
       " 'decision': 853,\n",
       " 'program': 854,\n",
       " 'style': 855,\n",
       " 'elizabeth': 856,\n",
       " 'names': 857,\n",
       " 'reveal': 858,\n",
       " 'workers': 859,\n",
       " 'advice': 860,\n",
       " 'iowa': 861,\n",
       " 'football': 862,\n",
       " 'surprise': 863,\n",
       " '13': 864,\n",
       " 'trans': 865,\n",
       " 'planned': 866,\n",
       " 'queer': 867,\n",
       " 'church': 868,\n",
       " 'asking': 869,\n",
       " 'test': 870,\n",
       " 'steve': 871,\n",
       " 'weight': 872,\n",
       " 'executive': 873,\n",
       " 'spot': 874,\n",
       " 'door': 875,\n",
       " 'oscars': 876,\n",
       " 'cnn': 877,\n",
       " 'kardashian': 878,\n",
       " 'university': 879,\n",
       " 'allegations': 880,\n",
       " 'learn': 881,\n",
       " 'boys': 882,\n",
       " 'defense': 883,\n",
       " 'williams': 884,\n",
       " 'anyone': 885,\n",
       " 'demand': 886,\n",
       " 'dying': 887,\n",
       " 'shop': 888,\n",
       " 'train': 889,\n",
       " 'blood': 890,\n",
       " 'paris': 891,\n",
       " 'possible': 892,\n",
       " '2014': 893,\n",
       " 'hear': 894,\n",
       " 'members': 895,\n",
       " 'band': 896,\n",
       " 'rubio': 897,\n",
       " 'album': 898,\n",
       " 'uses': 899,\n",
       " 'apparently': 900,\n",
       " 'transgender': 901,\n",
       " 'building': 902,\n",
       " 'coworker': 903,\n",
       " 'pro': 904,\n",
       " 'awards': 905,\n",
       " 'plane': 906,\n",
       " 'brings': 907,\n",
       " 'major': 908,\n",
       " 'chicago': 909,\n",
       " 'queen': 910,\n",
       " 'reform': 911,\n",
       " 'general': 912,\n",
       " 'doctor': 913,\n",
       " 'eat': 914,\n",
       " 'grandma': 915,\n",
       " 'leading': 916,\n",
       " 'tour': 917,\n",
       " 'pick': 918,\n",
       " 'homeless': 919,\n",
       " 'economy': 920,\n",
       " '14': 921,\n",
       " 'teens': 922,\n",
       " 'table': 923,\n",
       " 'died': 924,\n",
       " 'remember': 925,\n",
       " 'culture': 926,\n",
       " 'throws': 927,\n",
       " 'bowl': 928,\n",
       " 'easy': 929,\n",
       " 'bathroom': 930,\n",
       " 'prevent': 931,\n",
       " 'simple': 932,\n",
       " 'act': 933,\n",
       " 'urges': 934,\n",
       " 'moms': 935,\n",
       " 'michelle': 936,\n",
       " 'peace': 937,\n",
       " 'green': 938,\n",
       " 'explains': 939,\n",
       " 'hits': 940,\n",
       " 'push': 941,\n",
       " 'private': 942,\n",
       " 'voice': 943,\n",
       " 'pregnant': 944,\n",
       " 'suggests': 945,\n",
       " 'google': 946,\n",
       " 'suicide': 947,\n",
       " 'avoid': 948,\n",
       " 'supporters': 949,\n",
       " '2017': 950,\n",
       " 'passes': 951,\n",
       " 'fear': 952,\n",
       " 'quietly': 953,\n",
       " 'christian': 954,\n",
       " 'try': 955,\n",
       " 'website': 956,\n",
       " 'address': 957,\n",
       " 'wait': 958,\n",
       " '25': 959,\n",
       " 'road': 960,\n",
       " 'sound': 961,\n",
       " 'review': 962,\n",
       " 'leads': 963,\n",
       " 'beauty': 964,\n",
       " 'industry': 965,\n",
       " 'reading': 966,\n",
       " 'recalls': 967,\n",
       " 'spend': 968,\n",
       " 'staff': 969,\n",
       " 'fighting': 970,\n",
       " 'voting': 971,\n",
       " 'rest': 972,\n",
       " 'voter': 973,\n",
       " \"mother's\": 974,\n",
       " 'picture': 975,\n",
       " 'carolina': 976,\n",
       " 'mayor': 977,\n",
       " \"hasn't\": 978,\n",
       " 'hopes': 979,\n",
       " 'force': 980,\n",
       " 'racist': 981,\n",
       " 'epa': 982,\n",
       " 'walking': 983,\n",
       " 'finding': 984,\n",
       " 'families': 985,\n",
       " 'onion': 986,\n",
       " 'driver': 987,\n",
       " 'poor': 988,\n",
       " 'crowd': 989,\n",
       " 'happens': 990,\n",
       " 'reminds': 991,\n",
       " 'receives': 992,\n",
       " 'problems': 993,\n",
       " 'lets': 994,\n",
       " 'netflix': 995,\n",
       " 'card': 996,\n",
       " 'desperate': 997,\n",
       " 'magazine': 998,\n",
       " 'ben': 999,\n",
       " 'protesters': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "2fda1579",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seq = pad_sequences(token.texts_to_sequences(headlines_train), maxlen=50, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "59b5d85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1, 340,   1, ...,   0,   0,   0],\n",
       "       [  1,   1, 721, ...,   0,   0,   0],\n",
       "       [914,  33,   1, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  1,   1,   1, ...,   0,   0,   0],\n",
       "       [212, 757, 464, ...,   0,   0,   0],\n",
       "       [  1, 240,   1, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "f61cf8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_seq = pad_sequences(token.texts_to_sequences(headlines_test), maxlen=50, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "de28c577",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32,   1, 390, ...,   0,   0,   0],\n",
       "       [  1,   1,   1, ...,   0,   0,   0],\n",
       "       [300, 623,   1, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  4, 100, 640, ...,   0,   0,   0],\n",
       "       [  1,   1,   1, ...,   0,   0,   0],\n",
       "       [215,   1,  21, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "218e902e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert labels into array \n",
    "train_labels = np.array(labels_train)\n",
    "test_labels = np.array(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ba52e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "577c6c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Embedding, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0034f3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5b654a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layer\n",
    "model.add(Embedding(1000, input_length=50, output_dim = 16))\n",
    "\n",
    "#first hidden layer\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "#second hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "#model.add(GlobalAveragePooling2D())\n",
    "#output layer\n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "29b523fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_6 (Embedding)     (None, 50, 16)            16000     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 50, 128)           2176      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 50, 128)           0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 50, 64)            8256      \n",
      "                                                                 \n",
      " dropout_11 (Dropout)        (None, 50, 64)            0         \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 1)                 3201      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 29,633\n",
      "Trainable params: 29,633\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "bf6d75a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model \n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8789f69",
   "metadata": {},
   "source": [
    "#### Model Testing\n",
    "After training, evaluate the model on a separate test set to ensure its generalization to unseen data. Analyze the confusion matrix and performance metrics to understand how well the model is distinguishing between sarcastic and non-sarcastic sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "fb43adbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "805/805 [==============================] - 11s 11ms/step - loss: 0.4465 - accuracy: 0.7721 - val_loss: 0.3965 - val_accuracy: 0.8071\n",
      "Epoch 2/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.3598 - accuracy: 0.8355 - val_loss: 0.3767 - val_accuracy: 0.8246\n",
      "Epoch 3/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.3478 - accuracy: 0.8417 - val_loss: 0.3722 - val_accuracy: 0.8284\n",
      "Epoch 4/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.3344 - accuracy: 0.8483 - val_loss: 0.3674 - val_accuracy: 0.8312\n",
      "Epoch 5/10\n",
      "805/805 [==============================] - 8s 10ms/step - loss: 0.3253 - accuracy: 0.8518 - val_loss: 0.3716 - val_accuracy: 0.8312\n",
      "Epoch 6/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.3218 - accuracy: 0.8536 - val_loss: 0.3733 - val_accuracy: 0.8309\n",
      "Epoch 7/10\n",
      "805/805 [==============================] - 8s 10ms/step - loss: 0.3148 - accuracy: 0.8587 - val_loss: 0.3699 - val_accuracy: 0.8302\n",
      "Epoch 8/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.3096 - accuracy: 0.8603 - val_loss: 0.3719 - val_accuracy: 0.8267\n",
      "Epoch 9/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.3051 - accuracy: 0.8627 - val_loss: 0.3783 - val_accuracy: 0.8284\n",
      "Epoch 10/10\n",
      "805/805 [==============================] - 9s 11ms/step - loss: 0.2984 - accuracy: 0.8663 - val_loss: 0.3740 - val_accuracy: 0.8239\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25789bd9f10>"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train the model\n",
    "model.fit(train_seq,train_labels, epochs=10, validation_data=(test_seq,test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a3f179",
   "metadata": {},
   "source": [
    "#### Predicting User Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "2a172679",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = ['Where are women judges in highcourts ?']\n",
    "test = pad_sequences(token.texts_to_sequences(test), maxlen=50, padding='post', truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "1a2becc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 260ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.]], dtype=float32)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "e553c7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test2 = ['teacher strikes idle kids']\n",
    "test2 = pad_sequences(token.texts_to_sequences(test2), maxlen=50, padding='post',truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8b29b9cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 39ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.]], dtype=float32)"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(test2).round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "26e51119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 78ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(model.predict(test2).round())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8570a6",
   "metadata": {},
   "source": [
    "###### Taking headline as an input from the user and predicting whether headline is sarcastic or not \n",
    "Now that the model is trained and tested, it's time to deploy it for real-world use. Create a simple user interface to takeinputs from users. Tokenize and preprocess the user input, then feed it into the trained model for prediction. Provide users with a clear indication of whether the input is sarcastic or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "044bba71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter headline for prediction (Type 'stop' to break the loop): War Dims Hope for Peace\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "****************************************\n",
      "Provided headline is Sarcastic\n",
      "****************************************\n",
      "Enter headline for prediction (Type 'stop' to break the loop): Mahua Moitra: Expelled from Parliament over cash-for-query row\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "****************************************\n",
      "Provided headline is NOT Sarcastic\n",
      "****************************************\n",
      "Enter headline for prediction (Type 'stop' to break the loop): stop\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    head1 = []\n",
    "    str1 = str(input(\"Enter headline for prediction (Type 'stop' to break the loop): \"))\n",
    "    # Check if the user wants to stop\n",
    "    if str1.lower() == 'stop':\n",
    "        break  \n",
    "    else:\n",
    "        \n",
    "        head1.extend([str1])\n",
    "        head1 = pad_sequences(token.texts_to_sequences(head1), maxlen=50, padding='post', truncating='post')\n",
    "        temp = (model.predict(head1)).round()\n",
    "        #int(temp) = temp.round()\n",
    "        #print(head1, type(head1))\n",
    "\n",
    "\n",
    "        if int(temp) == 1:\n",
    "            print(\"****************************************\")\n",
    "            print(\"Provided headline is Sarcastic\")\n",
    "            print(\"****************************************\")\n",
    "        else:\n",
    "            print(\"****************************************\")\n",
    "            print(\"Provided headline is NOT Sarcastic\")\n",
    "            print(\"****************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc979848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71342231",
   "metadata": {},
   "source": [
    "Sentences to test : <br>\n",
    "War Dims Hope for Peace <br>\n",
    "Mahua Moitra: Expelled from Parliament over cash-for-query row<br>\n",
    "Cold Wave Linked to Temperatures <br>   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b9c085",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4140b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
